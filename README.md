

# ğŸ“š Book Scraper with Scrapy and MongoDB

This project is a web scraping application built using **Scrapy** to extract book data from websites and store it in **MongoDB**.

---

## ğŸ“ Project Structure

```
books/
â”‚
â”œâ”€â”€ books/
â”‚   â”œâ”€â”€ spiders/
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ items.py
â”‚   â”œâ”€â”€ middlewares.py
â”‚   â”œâ”€â”€ pipelines.py
â”‚   â””â”€â”€ settings.py
â”‚
â””â”€â”€ scrapy.cfg
```

---

## ğŸ› ï¸ Prerequisites

Before running the project, ensure the following are installed:

- âœ… Python 3.x
- âœ… MongoDB
- âœ… Scrapy

---

## ğŸ”§ Setup Instructions

### 1. Create a Virtual Environment

```powershell
py -m venv venv
```

### 2. Activate the Virtual Environment

```powershell
venv\Scripts\activate
```

### 3. Install Scrapy

```powershell
(venv) pip install scrapy
```

### 4. Set Up MongoDB

Connect to MongoDB and create the database and collection:

```javascript
use books_db
db.createCollection("books")
```

---

## ğŸ•·ï¸ Running the Project

### Generate a New Spider (Optional)

Replace `book` and `example.com` with your desired spider name and domain:

```powershell
(venv) scrapy genspider book example.com
```

### Run the Spider

```powershell
(venv) scrapy crawl book
```

### Check Spider Contracts (Testing)

```powershell
(venv) scrapy check book
```

---

## ğŸ§° Available Scrapy Commands

| Command       | Description                              |
|---------------|------------------------------------------|
| `bench`       | Run quick benchmark test                 |
| `fetch`       | Fetch a URL using the Scrapy downloader  |
| `genspider`   | Generate new spider                      |
| `runspider`   | Run a self-contained spider              |
| `settings`    | Get settings values                      |
| `shell`       | Interactive scraping console             |
| `startproject`| Create new project                       |
| `version`     | Print Scrapy version                     |
| `view`        | Open URL in browser as seen by Scrapy    |

---

## âš™ï¸ Configuration

- Database settings can be configured in `settings.py`.
- Define item pipelines in `pipelines.py`.
- Implement spiders inside the `spiders/` directory.

---

## ğŸ§ª Testing

The project includes contract testing for spiders. To run tests:

```powershell
(venv) scrapy check book
```

---

## ğŸ’¾ Data Storage

All scraped book data is stored in MongoDB under the `books_db.books` collection.

Make sure MongoDB is running and the connection string is properly set in `settings.py`.

---

## ğŸ“˜ Notes

- Always respect website terms of service and robots.txt when scraping.
- Add appropriate middleware or delays if necessary to avoid overwhelming servers.

---

Happy scraping! ğŸ•µï¸â€â™‚ï¸ğŸ“–

--- 
